{"meta":{"title":"CherishMelon","subtitle":"若你喜欢怪人，其实我很美","description":"欢迎访问","author":"Cherish Melon","url":"http://example.com","root":"/"},"pages":[{"title":"categories","date":"2022-05-05T09:05:58.000Z","updated":"2022-05-05T09:06:16.272Z","comments":true,"path":"categories/index.html","permalink":"http://example.com/categories/index.html","excerpt":"","text":""},{"title":"Tags","date":"2022-05-05T04:15:59.000Z","updated":"2022-05-05T09:23:19.737Z","comments":true,"path":"tags/index.html","permalink":"http://example.com/tags/index.html","excerpt":"","text":""}],"posts":[{"title":"Pod","slug":"Pod","date":"2022-05-11T08:57:15.000Z","updated":"2022-05-11T09:10:23.263Z","comments":true,"path":"2022/05/11/Pod/","link":"","permalink":"http://example.com/2022/05/11/Pod/","excerpt":"","text":"pod基本概念 最小部署单元 一组容器的集合 一个pod中的容器共享网络命名空间 pod是短暂的应用场景 应用之间需要发生文件交互 应用之间需要通过127.0.0.1或者socket通信 应用之间发生频繁交互容器分类 Infrastructure Container：基础容器，维护整个Pod网络空间 — 在节点上利用docker ps | grep pause 可以看到 InitContainers：初始化容器，先于业务容器开始执行 Containers：业务容器，并行启动initContainer 基本支持所有普通容器特征 优先普通容器执行应用场景 1）控制普通容器启动，初始容器完成后才会启动业务容器； 2）初始化配置，例如下载应用配置文件、注册信息等； — 是通过共享存储卷的形式 apiVersion: v1kind: Podmetadata: name: init-demospec: containers: - name: nginx image: nginx ports: - containerPort: 80 volumeMounts: - name: workdir mountPath: /usr/share/nginx/html initContainers: - name: install image: busybox command: - wget - &quot;-O&quot; - &quot;/work-dir/index.html&quot; - http://kubernetes.io volumeMounts: - name: workdir mountPath: &quot;/work-dir&quot; dnsPolicy: Default volumes: - name: workdir emptyDir: &#123;&#125; 应用自恢复（重启策略+健康检查）重启策略● Always：当容器终止退出后，总是重启容器，默认策略● OnFailure：当容器异常退出（退出状态码非0）时，才重启容器● Never：当容器终止退出时，从不重启容器 健康检查 livenessProbe（存活检查） 如果检查失败，将杀死容器，根据POD的restartPolicy来操作 readnessProbe（就绪检查） 如果检查失败，kubernetes会把pod从service endpoints中剔除 支持以下三种检查方法： - httpGet：发送http请求，返回200~400范围内状态码为成功- exec：执行shell命令返回状态码是0则成功- tcpSocket：发起TCP Socket建立成功 apiVersion: apps/v1kind: Deploymentmetadata: name: java-checkspec: replicas: 2 selector: matchLabels: project: check app: java-check template: metadata: labels: project: check app: java-check spec: restartPolicy: Always containers: - name: java-check image: tomcat:8 livenessProbe:####tcpsocket方式 #tcpSocket: #port: 8080 #initialDelaySeconds: 30 #periodSeconds: 20#####exec-shell方式 #exec: #command: #- cat #- /tmp/healthy #initialDelaySeconds: 5 #periodSeconds: 5#####HTTPGet方式 httpGet: path: /index.html port: 8080 #启动后等待5秒钟开始检测 initialDelaySeconds: 5 #检测周期5s periodSeconds: 5 readnessProbe: httpGet: path: /index.html port: 8080 initialDelaySeconds: 5 periodSeconds: 5 静态pod1、固定在某个node上面，由kubelet管理生成的一种pod，无法使用控制器2、在kubelet配置文件启用静态pod： $ vi /var/lib/kubelet/config.yaml:......staticPodPtah：/etc/kubernetes/manifests/ 将部署的pod yaml放到该目录会由kubelet自动创建3、特点： 1）yaml只能是pod 2）pod是由kubelet拉起的，只管理当前节点上的yaml 3）kubelet会定期扫描静态pod目录，根据目录下的yaml启动或者删除 实验1、向pod中添加一个init容器，init容器创建一个空文件，如果该空文件没有被检测到pod退出 apiVersion: v1kind: Podmetadata: name: init-checkspec: containers: - name: init-check image: registry.cn-hangzhou.aliyuncs.com/cherish/gjh:centos-v1 volumeMounts: - name: workdir mountPath: /tmp livenessProbe: exec: command: - cat - /tmp/test initialDelaySeconds: 5 periodSeconds: 5 initContainers: - name: install image: busybox command: - touch - /tmp/test volumeMounts: - name: workdir mountPath: &quot;/tmp&quot; restartPolicy: Always volumes: - name: workdir emptyDir: &#123;&#125; 2、创建一个pod，其中运行着nginx、redis、memcached、consul 4个容器 apiVersion: v1kind: Podmetadata: name: duo-podspec: containers: - name: nginx image: nginx - name: redis image: redis - name: memcached image: memcached - name: consul image: consul","categories":[{"name":"Kubernetes","slug":"Kubernetes","permalink":"http://example.com/categories/Kubernetes/"},{"name":"Pod","slug":"Kubernetes/Pod","permalink":"http://example.com/categories/Kubernetes/Pod/"}],"tags":[{"name":"Kubernetes","slug":"Kubernetes","permalink":"http://example.com/tags/Kubernetes/"}],"author":null},{"title":"控制器","slug":"控制器","date":"2022-05-11T03:46:18.000Z","updated":"2022-05-11T09:17:54.553Z","comments":true,"path":"2022/05/11/控制器/","link":"","permalink":"http://example.com/2022/05/11/%E6%8E%A7%E5%88%B6%E5%99%A8/","excerpt":"","text":"控制器类型 ReplicationController与ReplicaSet Deployment DaemonSet StatefulSet Job/Cronjob Horizontal Pod Autoscaling ReplicationController与ReplicaSetReplicationController（RC）用来确保容器应用的副本数始终保持在用户自定义的副本数，即如果有容器异常退出，回自动创建新的pod来替代；而如果异常多出来的容器也会自动回收； 在新的版本的kubernetes中建议使用ReplicaSet来取代ReplicationController。ReplicaSet跟ReplicationController没有本质的不同，只是名字不一样，并且ReplicaSet支持集合式的selector； DeploymentDeployment为pod和replicaset提供了 声明式（declarative）方法，用来替代之前的ReplicationController来方便的管理应用。典型的应用场景包括： 定义Deployment来创建pod和ReplicaSet 滚动升级和回滚应用 扩容和缩容 暂停和继续DeploymentRS和Deployment的关联 部署Deployment创建Nginx-deploymentapiVersion: apps/v1kind: Deploymentmetadata: name: nginx-deploymentspec: replicas: 2 selector: matchLabels: app: nginx-deployment template: metadata: labels: app: nginx-deployment spec: containers: - name: nginx-deployment image: nginx:1.7.9 ports: - containerPort: 80 kubectl create -f nginx-deployment.yaml --record # --record可以记录命令，方便查看每次的revision 扩容kubectl scale deployment nginx-deployment --replicas 3# 如果集群支持Horizontal pod autoscaling 的话，还可以设置自动扩展kubectl autoscale deployment nginx-deployment --min=2 --max=3 --cpu-percent=80 更新Deployment镜像更新#方式一kubectl set image deployment/nginx-deployment ngin-deployment=nginx:1.9.1#方式二kubectl edit deployment/nginx-deployment 回滚$ kubectl rollout undo deployment/nginx-deploymentdeployment.apps/nginx-deployment rolled back 查看rollout的状态$ kubectl rollout status deployment/nginx-deploymentdeployment &quot;nginx-deployment&quot; successfully rolled out 查看历史RS$ kubectl get rsNAME DESIRED CURRENT READY AGEnginx-deployment-6588fd458d 2 2 2 25mnginx-deployment-6847fcfd4b 0 0 0 14m Deployment更新策略Deployment可以保证在升级时只有一定数量的Pod是down的。默认情况下，它会确保至少有比期望的Pod数量少一个是up状态（最多一个不可用） Deployment同时也可以确保只创建出超过期望数量的一定数量的Pod。默认情况下，它会确保最多比期望的Pod数量多一个是up状态（最多一个surge） Rollover（多个Rollout并行）假如你创建了一个有5个ngin:1.7.9 replica的Deployment，但是当时还只有3个nginx:1.7.9的replica创建出来的时候你就开始更新含有5个nginx:1.9.1 replica的Deployment。在这种情况下，Deployment会立即杀掉已创建的3个nginx:1.7.9的pod，并开始创建nginx:1.9.1的pod。他不会等到所有的5个nginx:1.7.9的pod都创建以后才开始更新 清理Policy你可以设置.spec.revisionHistoryLimit项来指定deployment最多保留多少revision历史记录。默认的会保留所有的revision；如果将该项设置为0，deployment就不允许回退 StatefulSetStatefulSet作为Controller为pod提供唯一的标识，他可以保证部署和scale的顺序StatefulSet是为了解决有状态服务的问题（对应Deployment和ReplicaSet是为无状态服务设计的），其应用场景包括： 稳定的持久化存储，即Pod重新调度后还是能访问到相同的持久化数据，基于PVC来实现 稳定的网络标志，即Pod重新调度后其PodName和HostName不变，基于Headless Service（即没有Cluster IP的Service）来实现 有序部署，有序扩展，即Pod是有顺序的，在部署或者扩展的时候要依据定义的顺序依次进行（即从0到N-1，在下一个Pod运行之前所有之前的Pod必须都是Running和Ready状态），基于init containers来实现 有序收缩，有序删除（即从N-1到0） 有状态应用 VS 无状态应用：主要在于网络和存储两方面 无状态应用，例如nginx。主要可以任意飘逸，每个副本是对等的有状态应用，例如etcd、zookeeper、mysql主从，每个副本是不对等 1、headless service：不同于平常service在于不需要cluster IP，也就是在yaml中定义spec.clusterIP为None apiVersion: v1kind: Servicemetadata: labels: app: headless-service name: headless-servicespec: clusterIP: None ports: - port: 80 protocol: TCP targetPort: 80 selector: app: headless-service---apiVersion: apps/v1kind: StatefulSetmetadata: name: statefulset-webspec: selector: matchLabels: app: headless-service serviceName: &quot;headless-service&quot; replicas: 2 template: metadata: labels: app: headless-service spec: containers: - name: headless-pod image: nginx ports: - containerPort: 80 name: headless-pod 上图中的pod的编号是0,1,对应启动顺序利用nslookup来确认statefulset唯一稳定的网络ID ClusterIP A记录格式： &lt;service-name&gt;.&lt;namespace-name&gt;.svc.cluster.localClusterIP=None A记录格式： &lt;statefulsetName-index&gt;.&lt;service-name&gt; .&lt;namespace-name&gt;.svc.cluster.local 示例：web-0.nginx.default.svc.cluster.local 上图中headless的service的dnsIP指定的就是每个pod的IP。但普通service的dnsIP 是对应的service的clusterIP 确认statefulset稳定的存储—-也就是每个pod都是独立的PV 1、StatefulSet的存储卷使用VolumeClaimTemplate创建， 称为卷申请模板，当StatefulSet使用VolumeClaimTemplate 创建一个PersistentVolume时，同样也会为每个Pod分配 并创建一个编号的PVC apiVersion: v1kind: Servicemetadata: labels: app: headless-service name: headless-servicespec: clusterIP: None ports: - port: 80 protocol: TCP targetPort: 80 selector: app: headless-service---apiVersion: apps/v1kind: StatefulSetmetadata: name: statefulset-webspec: selector: matchLabels: app: headless-service serviceName: &quot;headless-service&quot; replicas: 2 template: metadata: labels: app: headless-service spec: containers: - name: headless-pod image: nginx ports: - containerPort: 80 name: headless-pod volumeMounts: - name: www mountPath: /usr/share/nginx/html volumeClaimTemplates: - metadata: name: www spec: accessModes: [&quot;ReadWriteOnce&quot;] storageClassName: &quot;managed-nfs-storage&quot; resources: requests: storage: 1Gi 在k8s中部署有状态分布式应用主要解决的问题：1、通过一个镜像怎么自动化生成各自独立的配置文件2、部署这个应用在k8s中的一个拓扑图 DaemonSetDaemonSet确保全部（或者一些）node上运行一个pod的副本。当有node加入集群时，也为他们新增一个pod。当有node从集群移除时，这些pod也会被回收。删除的DaemonSet将会删除它创建的所有pod。 使用DaemonSet的一些典型用法： 运行集群存储daemon，例如在每个Node上运行glusterd、ceph 在每个Node上运行日志收集daemon，例如fluentd、logstash 在每个Node上运行监控daemon，例如Prometheus Node Exporter、collectd、datadog代理、New Relic代理，或Ganglia gmodapiVersion: apps/v1kind: DaemonSetmetadata: name: fluentd-elasticsearch namespace: kube-system labels: k8s-app: fluentd-loggingspec: selector: matchLabels: name: fluentd-elasticsearch template: metadata: labels: name: fluentd-elasticsearch spec: tolerations: # this toleration is to have the daemonset runnable on master nodes # remove it if your masters can&#x27;t run pods - key: node-role.kubernetes.io/master operator: Exists effect: NoSchedule containers: - name: fluentd-elasticsearch image: quay.io/fluentd_elasticsearch/fluentd:v2.5.2 resources: limits: memory: 200Mi requests: cpu: 100m memory: 200Mi volumeMounts: - name: varlog mountPath: /var/log - name: varlibdockercontainers mountPath: /var/lib/docker/containers readOnly: true terminationGracePeriodSeconds: 30 volumes: - name: varlog hostPath: path: /var/log - name: varlibdockercontainers hostPath: path: /var/lib/docker/containers JobJob负责批处理任务，即仅执行一次的任务，它保证批处理任务的一个或多个pod成功结束1、特殊说明 spec.template 格式同Pod RestartPolicy仅支持Nerver或者OnFailure 单个Pod时，默认Pod成功运行后Job即结束 .spec.completions标志Job结束需要成功运行的Pod个数，默认为1 .spec.parallelism标志并行运行的Pod的个数，默认为1 .spec.activeDeadlineSeconds标志失败Pod的重试最大时间，超过这个时间不会继续重试 .spec.ttlSecondsAfterFinished自动清理完成的Job的TTL机制，如果该字段设置为 0，Job 在结束之后立即成为可被自动删除的对象。 如果该字段没有设置，Job 不会在结束之后被 TTL 控制器自动清除2、示例：apiVersion: batch/v1kind: Jobmetadata: name: pispec: template: spec: containers: - name: pi image: perl command: [&quot;perl&quot;, &quot;-Mbignum=bpi&quot;, &quot;-wle&quot;, &quot;print bpi(2000)&quot;] restartPolicy: Never backoffLimit: 4 #backoffLimit表示回退限制，失效回退的限制值默认为 6，可以指定重试几次后将 Job 标记为失败 以机器可读的方式列举属于某job的全部pods,可以使用：pods=$(kubectl get pods –selector=job-name=pi –output=jsonpath=’{.items[*].metadata.name}’);echo $pods，查看其中某个Pod的标准输出：kubectl logs $pods CronJobCronJob管理基于时间的job，即： 在给定时间点至运行一次 周期性的在给定时间点运行使用前提条件：当前使用的kubernetes集群，版本&gt;=1.8(对CronJob)。对于先前版本的集群，版本&lt;1.8，启动ApiServer时，通过传递选项–runtime-config=batch/v2alpha1API 典型的用法如下所示： 在给定的时间点调度Job运行 创建周期性运行的Job，例如：数据库备份、发生邮件 1、特殊说明 .spec.schedule ：调度，必需字段，指定运行周期，格式同Cron .spec.jobTemplate：Job模板，必需字段，指定需要运行的任务，格式同Job .spec.startingDeadlineSeconds：启动Job的期限（秒级别），该字段是可选的。如果因为任何原因错过了被调度的时间，那么错过执行时间的Job将会被认为是失败的；如果没有指定，则没有期限 .spec.concurrencyPolicy：并发策略，该字段也是可选的。他指定了如何处理被Cron Job创建的Job的并发执行，只允许下面策略的一种： Allow（默认）：允许并发允许Job Forbid：禁止并发执行，如果前一个没有完成，则直接跳过下一个 Replace：取消当前正在允许的Job，用一个新的替换== 注意，当前策略只能应用于同一个Cron Job创建的Job。如果存在多个Cron Job，他们创建的Job之间总是允许并发执行的 .spec.suspend：挂起，该字段也是可选的。如果设置为TRUE，后续所有的执行都会被挂起，他对已经开始执行的Job不起作用。默认值为false .spec.successfulJobsHistoryLimit和.spec.failedJobsHistoryLimit：历史限制，是可选字段，他们指定了可以保留多少完成和失败的Job。默认情况下，他们分别设置为3和1，设置限制的值为0，相关类型的Job完成后将不会保留2、示例：apiVersion: batch/v1kind: CronJobmetadata: name: hellospec: schedule: &quot;*/1 * * * *&quot; jobTemplate: spec: template: spec: containers: - name: hello image: busybox imagePullPolicy: IfNotPresent command: - /bin/sh - -c - date; echo Hello from the Kubernetes cluster restartPolicy: OnFailure $ kubectl get cronjobNAME SCHEDULE SUSPEND ACTIVE LAST SCHEDULE AGEhello */1 * * * * False 1 3s 6m50s$ kubectl get jobsNAME COMPLETIONS DURATION AGEhello-1628066820 1/1 1s 2m11shello-1628066880 1/1 2s 71shello-1628066940 1/1 1s 10s$ pods=$(kubectl get pods --selector=job-name=hello-1628066760 --output=jsonpath=&#x27;&#123;.items[*].metadata.name&#125;&#x27;);echo $podshello-1628066760-z5kqb$ kubectl logs $podsWed Aug 4 08:46:02 UTC 2021Hello from the Kubernetes cluster#注意：删除cornjob的时候不会自动删除job，可以使用kubectl delete job 来删除 Horizontal Pod Autoscaling应用的资源使用率通常都有高峰和低谷的时候，如何削峰填谷，提高集群的整体资源利用率，让service中的Pod个数自动调整呢？这就依赖于Horizontal Pod Autoscaling了，顾名思义，使Pod水平自动缩放https://www.yuque.com/cherishmelon/xy3e1p/agza6z?inner=79ebf1b6","categories":[{"name":"Kubernetes","slug":"Kubernetes","permalink":"http://example.com/categories/Kubernetes/"},{"name":"控制器","slug":"Kubernetes/控制器","permalink":"http://example.com/categories/Kubernetes/%E6%8E%A7%E5%88%B6%E5%99%A8/"}],"tags":[{"name":"Kubernetes","slug":"Kubernetes","permalink":"http://example.com/tags/Kubernetes/"}],"author":null},{"title":"Kubernetes集群部署","slug":"Kubernetes集群部署","date":"2022-05-09T09:07:51.000Z","updated":"2022-05-11T09:12:44.882Z","comments":true,"path":"2022/05/09/Kubernetes集群部署/","link":"","permalink":"http://example.com/2022/05/09/Kubernetes%E9%9B%86%E7%BE%A4%E9%83%A8%E7%BD%B2/","excerpt":"","text":"概念官方文档：www.kubernetes.io 博客文章：https://www.cnblogs.com/menkeyi/p/7134460.html 架构Master节点：- kube-apiserver- controller-manager- scheduler- etcdNode节点：- kubelet- kube-proxy- Docker kudeadm安装kubeadm是官方社区推出的一个用于快速部署kubernetes集群的工具。这个工具能通过两条指令完成一个kubernetes集群的部署： # 创建一个 Master 节点$ kubeadm init# 将一个 Node 节点加入到当前集群中$ kubeadm join &lt;Master节点的IP和端口 安装要求在开始之前，部署Kubernetes集群机器需要满足以下几个条件： 一台或多台机器，操作系统 CentOS7.x-86_x64 硬件配置：2GB或更多RAM，2个CPU或更多CPU，硬盘30GB或更多 集群中所有机器之间网络互通 可以访问外网，需要拉取镜像 禁止swap分区准备环境角色 IPk8s-master 192.168.31.61k8s-node1 192.168.31.62k8s-node2 192.168.31.63关闭防火墙：```shell$ systemctl stop firewalld$ systemctl disable firewalld关闭selinux：$ sed -i &#x27;s/enforcing/disabled/&#x27; /etc/selinux/config # 永久$ setenforce 0 # 临时关闭swap：$ swapoff -a # 临时$ vim /etc/fstab # 永久设置主机名：$ hostnamectl set-hostname &lt;hostname&gt;在master添加hosts：$ cat &gt;&gt; /etc/hosts &lt;&lt; EOF192.168.31.61 k8s-master192.168.31.62 k8s-node1192.168.31.63 k8s-node2EOF将桥接的IPv4流量传递到iptables的链：$ cat &gt; /etc/sysctl.d/k8s.conf &lt;&lt; EOFnet.bridge.bridge-nf-call-ip6tables = 1net.bridge.bridge-nf-call-iptables = 1EOF$ sysctl --system # 生效时间同步：$ yum install ntpdate -y$ ntpdate time.windows.com 所有节点安装Docker/kubeadm/kubeletKubernetes默认CRI（容器运行时）为Docker，因此先安装Docker。 安装Docker$ wget https://mirrors.aliyun.com/docker-ce/linux/centos/docker-ce.repo -O /etc/yum.repos.d/docker-ce.repo$ yum -y install docker-ce-18.06.1.ce-3.el7$ systemctl enable docker &amp;&amp; systemctl start docker$ docker --versionDocker version 18.06.1-ce, build e68fc7a 添加# cat &gt; /etc/docker/daemon.json &lt;&lt; EOF&#123; &quot;registry-mirrors&quot;: [&quot;https://b9pmyelo.mirror.aliyuncs.com&quot;]&#125;EOF 之后需要重启docker，利用docker info查看配置的加速源是否生效 添加阿里云YUM软件源$ cat &gt; /etc/yum.repos.d/kubernetes.repo &lt;&lt; EOF[kubernetes]name=Kubernetesbaseurl=https://mirrors.aliyun.com/kubernetes/yum/repos/kubernetes-el7-x86_64enabled=1gpgcheck=0repo_gpgcheck=0gpgkey=https://mirrors.aliyun.com/kubernetes/yum/doc/yum-key.gpg https://mirrors.aliyun.com/kubernetes/yum/doc/rpm-package-key.gpgEOF ubuntu系统配置源并安装： https://kubernetes.io/zh/docs/setup/production-environment/tools/kubeadm/install-kubeadm/#%E5%AE%89%E8%A3%85-kubeadm-kubelet-%E5%92%8C-kubectl 安装kubeadm，kubelet和kubectl由于版本更新频繁，这里指定版本号部署： $ yum install -y kubelet-1.18.0 kubeadm-1.18.0 kubectl-1.18.0$ systemctl enable kubelet 部署Kubernetes Master参考文档： https://kubernetes.io/zh/docs/reference/setup-tools/kubeadm/kubeadm-init/#config-file https://kubernetes.io/docs/setup/production-environment/tools/kubeadm/create-cluster-kubeadm/#initializing-your-control-plane-node 在192.168.31.61（Master）执行。 $ kubeadm init \\ --apiserver-advertise-address=192.168.31.61 \\ --image-repository registry.aliyuncs.com/google_containers \\ --kubernetes-version v1.18.0 \\ --service-cidr=10.96.0.0/12 \\ --pod-network-cidr=10.244.0.0/16 \\ --ignore-preflight-errors=all 由于默认拉取镜像地址k8s.gcr.io国内无法访问，这里指定阿里云镜像仓库地址。 或者使用配置文件引导： $ vi kubeadm.confapiVersion: kubeadm.k8s.io/v1beta2kind: ClusterConfigurationkubernetesVersion: v1.18.0imageRepository: registry.aliyuncs.com/google_containers networking: podSubnet: 10.244.0.0/16 serviceSubnet: 10.96.0.0/12 $ kubeadm init --config kubeadm.conf --ignore-preflight-errors=all 使用kubectl工具： mkdir -p $HOME/.kubesudo cp -i /etc/kubernetes/admin.conf $HOME/.kube/configsudo chown $(id -u):$(id -g) $HOME/.kube/config$ kubectl get nodes 加入Kubernetes Nodehttps://kubernetes.io/zh/docs/reference/setup-tools/kubeadm/kubeadm/ 在192.168.31.62/63（Node）执行。 向集群添加新节点，执行在kubeadm init输出的kubeadm join命令： $ kubeadm join 192.168.31.61:6443 --token esce21.q6hetwm8si29qxwn \\ --discovery-token-ca-cert-hash sha256:00603a05805807501d7181c3d60b478788408cfe6cedefedb1f97569708be9c5 默认token有效期为24小时，当过期之后，该token就不可用了。这时就需要重新创建token，操作如下： # kubeadm token create# kubeadm token list# openssl x509 -pubkey -in /etc/kubernetes/pki/ca.crt | openssl rsa -pubin -outform der 2&gt;/dev/null | openssl dgst -sha256 -hex | sed &#x27;s/^.* //&#x27;63bca849e0e01691ae14eab449570284f0c3ddeea590f8da988c07fe2729e924# kubeadm join 192.168.31.61:6443 --token nuja6n.o3jrhsffiqs9swnu --discovery-token-ca-cert-hash sha256:63bca849e0e01691ae14eab449570284f0c3ddeea590f8da988c07fe2729e924 kubeadm token create –print-join-command https://kubernetes.io/docs/reference/setup-tools/kubeadm/kubeadm-join/ 网络方案（CNI）https://kubernetes.io/docs/setup/production-environment/tools/kubeadm/create-cluster-kubeadm/#pod-network 注意：只需要部署下面其中一个，推荐Calico。 CalicoCalico是一个纯三层的数据中心网络方案，Calico支持广泛的平台，包括Kubernetes、OpenStack等。 Calico 在每一个计算节点利用 Linux Kernel 实现了一个高效的虚拟路由器（ vRouter） 来负责数据转发，而每个 vRouter 通过 BGP 协议负责把自己上运行的 workload 的路由信息向整个 Calico 网络内传播。 此外，Calico 项目还实现了 Kubernetes 网络策略，提供ACL功能。 https://docs.projectcalico.org/getting-started/kubernetes/quickstart wget https://docs.projectcalico.org/manifests/calico.yaml 下载完后还需要修改里面配置项： 定义Pod网络（CALICO_IPV4POOL_CIDR），与前面pod CIDR配置一样 选择工作模式（CALICO_IPV4POOL_IPIP），支持BGP（Never）、IPIP（Always）、CrossSubnet（开启BGP并支持跨子网） 修改完后应用清单： # kubectl apply -f calico.yaml# kubectl get pods -n kube-system FlannelFlannel是CoreOS维护的一个网络组件，Flannel为每个Pod提供全局唯一的IP，Flannel使用ETCD来存储Pod子网与Node IP之间的关系。flanneld守护进程在每台主机上运行，并负责维护ETCD信息和路由数据包。 wget https://raw.githubusercontent.com/coreos/flannel/master/Documentation/kube-flannel.ymlsed -i -r &quot;s#quay.io/coreos/flannel:.*-amd64#lizhenliang/flannel:v0.11.0-amd64#g&quot; kube-flannel.yml 修改国内镜像仓库。 测试kubernetes集群 验证Pod工作 验证Pod网络通信 验证DNS解析在Kubernetes集群中创建一个pod，验证是否正常运行：$ kubectl create deployment nginx --image=nginx$ kubectl expose deployment nginx --port=80 --type=NodePort$ kubectl get pod,svc 访问地址：http://NodeIP:Port测试DNS：kubectl run dns-test -it –rm –image=busybox:1.28.4 – sh，进去pod后nslookup 百度地址部署 Dashboard $ wget https://raw.githubusercontent.com/kubernetes/dashboard/v2.0.3/aio/deploy/recommended.yaml 默认Dashboard只能集群内部访问，修改Service为NodePort类型，暴露到外部： kind: ServiceapiVersion: v1metadata: labels: k8s-app: kubernetes-dashboard name: kubernetes-dashboard namespace: kubernetes-dashboardspec: ports: - port: 443 targetPort: 8443 nodePort: 30001 selector: k8s-app: kubernetes-dashboard type: NodePort 访问地址：https://NodeIP:30001创建service account并绑定默认cluster-admin管理员集群角色： kubectl create serviceaccount dashboard-admin -n kube-systemkubectl create clusterrolebinding dashboard-admin --clusterrole=cluster-admin --serviceaccount=kube-system:dashboard-adminkubectl describe secrets -n kube-system $(kubectl -n kube-system get secret | awk &#x27;/dashboard-admin/&#123;print $1&#125;&#x27;) 使用输出的token登录Dashboard。后续如果忘记的话可以， kubectl get secrets -n kube-system找到对应的dashboard-admin-token-dhpnm名字的账号后，利用kubectl describe secrets dashboard-admin-token-dhpnm -n kube-system获取token进行登录","categories":[{"name":"Kubernetes","slug":"Kubernetes","permalink":"http://example.com/categories/Kubernetes/"}],"tags":[{"name":"Kubernetes","slug":"Kubernetes","permalink":"http://example.com/tags/Kubernetes/"}],"author":null},{"title":"Hello World","slug":"hello-world","date":"2022-05-05T02:28:05.762Z","updated":"2022-05-05T02:26:14.082Z","comments":true,"path":"2022/05/05/hello-world/","link":"","permalink":"http://example.com/2022/05/05/hello-world/","excerpt":"","text":"Welcome to Hexo! This is your very first post. Check documentation for more info. If you get any problems when using Hexo, you can find the answer in troubleshooting or you can ask me on GitHub. Quick StartCreate a new post$ hexo new &quot;My New Post&quot; More info: Writing Run server$ hexo server More info: Server Generate static files$ hexo generate More info: Generating Deploy to remote sites$ hexo deploy More info: Deployment","categories":[],"tags":[]}],"categories":[{"name":"Kubernetes","slug":"Kubernetes","permalink":"http://example.com/categories/Kubernetes/"},{"name":"Pod","slug":"Kubernetes/Pod","permalink":"http://example.com/categories/Kubernetes/Pod/"},{"name":"控制器","slug":"Kubernetes/控制器","permalink":"http://example.com/categories/Kubernetes/%E6%8E%A7%E5%88%B6%E5%99%A8/"}],"tags":[{"name":"Kubernetes","slug":"Kubernetes","permalink":"http://example.com/tags/Kubernetes/"}]}